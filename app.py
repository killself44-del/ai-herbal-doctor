import streamlit as st
import os
import requests
import json
from pinecone import Pinecone
from dotenv import load_dotenv

# 1. ë¡œì»¬ í™˜ê²½(.env) ë¡œë“œ
load_dotenv()

# 2. ì•ˆì „í•œ í‚¤ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ (ì—ëŸ¬ ë°©ì§€ìš©)
def get_secret(key_name):
    # 1ìˆœìœ„: Streamlit Cloud Secrets ì‹œë„
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except Exception:
        pass  # Secretsê°€ ì—†ì–´ë„ ì—ëŸ¬ ë‚´ì§€ ë§ê³  ë„˜ì–´ê°€!
    
    # 2ìˆœìœ„: ë¡œì»¬ í™˜ê²½ë³€ìˆ˜ ì‹œë„
    return os.getenv(key_name)

GOOGLE_API_KEY = get_secret("GOOGLE_API_KEY")
PINECONE_API_KEY = get_secret("PINECONE_API_KEY")

# 3. í‚¤ê°€ ì—†ìœ¼ë©´ ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì£¼ê¸°
if not GOOGLE_API_KEY or not PINECONE_API_KEY:
    st.error("ğŸš¨ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! Streamlit Cloudì˜ [Settings] -> [Secrets]ì— í‚¤ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# Configure Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "herb-knowledge"
index = pc.Index(index_name)

# Constants
GEMINI_EMBED_MODEL = "models/text-embedding-004"
GEMINI_GEN_MODEL = "gemini-2.0-flash-exp"
INTERVIEW_TURNS = 4 # Increased for deeper symptom analysis

# --- Helper Functions (REST API) ---
# --- Helper Functions (REST API) ---

def get_gemini_embedding(text):
    """Get embedding using Gemini REST API."""
    url = f"https://generativelanguage.googleapis.com/v1beta/{GEMINI_EMBED_MODEL}:embedContent?key={GOOGLE_API_KEY}"
    payload = {
        "model": GEMINI_EMBED_MODEL,
        "content": {"parts": [{"text": text}]}
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']['values']
    except Exception as e:
        st.error(f"Embedding Error: {e}")
        return None

def generate_gemini_response(messages, system_instruction):
    """
    Generate response using Gemini REST API.
    Args:
        messages: List of {"role": str, "content": str}
        system_instruction: String
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_GEN_MODEL}:generateContent?key={GOOGLE_API_KEY}"
    
    # Construct strictly interleaved content: User -> Model -> User ...
    # System instruction can be passed in 'system_instruction' field for gemini-1.5/2.0
    
    formatted_contents = []
    
    # Add history
    for msg in messages:
        role = "user" if msg["role"] == "user" else "model"
        formatted_contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })

    payload = {
        "system_instruction": {
            "parts": [{"text": system_instruction}]
        },
        "contents": formatted_contents
    }
    
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        data = response.json()
        return data['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Error generating response: {e}\nRaw Response: {response.text if 'response' in locals() else 'None'}"

def retrieve_context(query, top_k=3):
    """Retrieve relevant documents from Pinecone."""
    try:
        embedding = get_gemini_embedding(query)
        if not embedding:
            return ""
        
        search_results = index.query(
            vector=embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        contexts = []
        for match in search_results['matches']:
            meta = match['metadata']
            text = f"- ì´ë¦„: {meta.get('name', 'Unknown')}\n  íš¨ëŠ¥: {meta.get('efficacy', 'ì •ë³´ ì—†ìŒ')}\n  ì‚¬ìš©ë²•: {meta.get('usage', 'ì •ë³´ ì—†ìŒ')}\n  ì£¼ì˜ì‚¬í•­: {meta.get('caution', 'ì •ë³´ ì—†ìŒ')}"
            contexts.append(text)
            
        return "\n\n".join(contexts)
    except Exception as e:
        st.error(f"Search Error: {e}")
        return ""

# --- System Prompts ---

PROMPT_QUERY_REFINEMENT_DUAL = """
ë‹¹ì‹ ì€ 'ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
Pinecone ê²€ìƒ‰ì„ ìœ„í•´ **ë‚´ë³µì•½ìš©**ê³¼ **ì™¸ìš©ì•½ìš©** ë‘ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ê°ê° ìƒì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. **Line 1 (ë‚´ë³µì•½)**: ì¦ìƒ ìƒì„¸(80%) + ì²´ì§ˆ ë³´ì™„(20%)
   - **ì¤‘ìš”**: ë‹¨ìˆœíˆ 'ë‘í†µ'ì´ ì•„ë‹ˆë¼, 'ë’·ëª© í†µì¦', 'ìš±ì‹ ê±°ë¦¼', 'ì†Œí™”ë¶ˆëŸ‰ ë™ë°˜' ë“± êµ¬ì²´ì ì¸ ì–‘ìƒì„ í¬í•¨í•˜ì„¸ìš”.
   - ì˜ˆ: "ë’·ëª© ë»£ë»£í•¨ ê¸´ì¥ì„± ë‘í†µ ê°ˆê·¼ (ì†ŒìŒì¸ ëª¸ì´ ì°¸)"
2. **Line 2 (ì™¸ìš©ì•½)**: ì¦ìƒ ì™„í™” + ì°œì§ˆ/ì•„ë¡œë§ˆ í‚¤ì›Œë“œ
   - ì˜ˆ: "ë‘í†µ ì¿¨ë§ êµ­í™” ë°•í•˜ ì°œì§ˆ (ì§„ì • íš¨ê³¼)"

[ëŒ€í™” ë‚´ìš©]
{history}

[ê²°ê³¼]
ì„¤ëª… ì—†ì´ ì˜¤ì§ **ë‘ ì¤„ì˜ ë¬¸ìì—´ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
"""

PROMPT_INTERVIEW = """
ë‹¹ì‹ ì€ ë§¤ìš° ê¼¼ê¼¼í•œ 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” **[ì‹¬ì¸µ ë¬¸ì§„(Deep Interview) ë‹¨ê³„]**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” í™˜ìì˜ 'ì£¼ì¦ìƒ'ì„ í˜„ë¯¸ê²½ ë³´ë“¯ ìì„¸íˆ íŒŒì•…í•œ ë’¤, ë³´ì¡°ì ìœ¼ë¡œ ì²´ì§ˆì„ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ë¬¸ì§„ ìˆœì„œ ë° ì§€ì¹¨]
1. **Turn 1~2 (ì¦ìƒ ì‹¬ì¸µ íŒŒì•…)**:
   - ì£¼ì¦ìƒì˜ **ìœ„ì¹˜** (ì˜ˆ: ë¨¸ë¦¬ ì•/ë’¤/ì˜†, ë°° ìœ„/ì•„ë˜)
   - **í†µì¦ì˜ ì–‘ìƒ** (ì˜ˆ: ì½•ì½• ì‘¤ì‹¬, ë¬µì§í•¨, ë‹¹ê¹€, ì‹œë¦¼)
   - **ì•…í™”/ì™„í™” ìš”ì¸** (ì˜ˆ: ì°¬ ë°”ëŒ ë§ìœ¼ë©´ ì‹¬í•´ì§, ë°¤ì— ì‹¬í•´ì§, ìŠ¤íŠ¸ë ˆìŠ¤)
   - *ì£¼ì˜*: ì²´ì§ˆ(ì¶”ìœ„, ì†Œí™”)ì€ ì•„ì§ ë¬»ì§€ ë§ˆì„¸ìš”. ì¦ìƒë¶€í„° íŒŒì‹­ì‹œì˜¤.

2. **Turn 3~4 (ì „ì‹  ìƒíƒœ ë° ì²´ì§ˆ)**:
   - ì¦ìƒì´ íŒŒì•…ëœ í›„, ì†Œí™”/ëŒ€ë³€/ì¶”ìœ„íƒ/ìˆ˜ë©´ ë“±ì„ ë¬¼ì–´ ì²´ì§ˆì„ ì¶”ë¡ í•˜ì„¸ìš”.

3. **ê³µí†µ ì§€ì¹¨**:
   - í•œ ë²ˆì— 1~2ê°œì˜ ì§ˆë¬¸ë§Œ í•˜ì„¸ìš”.
   - "ì•„ê¹Œ ë’·ëª©ì´ ë‹¹ê¸´ë‹¤ê³  í•˜ì…¨ëŠ”ë°..." ì²˜ëŸ¼ í™˜ìì˜ ë§ì„ ì¸ìš©í•˜ì—¬ ê³µê°ëŒ€(Rapport)ë¥¼ í˜•ì„±í•˜ì„¸ìš”.
"""

PROMPT_PRESCRIPTION_EXPERT = """
ë‹¹ì‹ ì€ ëª…ì˜(åé†«) 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” ì§„ë‹¨ ë° ì²˜ë°© ë‹¨ê³„ì…ë‹ˆë‹¤. **ë‚´ë³µì•½**ê³¼ **ì™¸ìš©ì•½**ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë°©í•˜ì„¸ìš”.

[ì…ë ¥ ë°ì´í„°]
1. ë‚´ë³µì•½ í›„ë³´(Internal Context): {context_internal}
2. ì™¸ìš©ì•½ í›„ë³´(External Context): {context_external}
3. í™˜ì ì£¼ì¦ìƒ: {chief_complaint}

[ì§€ì¹¨]
1. **ê²€ì¦ ë‹¨ê³„(Self-Reflection)**:
   - **ë‚´ë³µì•½**: ì£¼ì¦ìƒì— íš¨ê³¼ê°€ ìˆê³  ì²´ì§ˆì— ë§ëŠ” ì•½ì´ˆ ì„ íƒ.
   - **ğŸ”´ ì ˆëŒ€ ê¸ˆê¸°**: 
     - **ì‹í’ˆì„± ì•½ì¬(ìŒ€, ëŒ€ì¶”, ê°ì´ˆ, ìƒê°• ë“±)** ë§Œìœ¼ë¡œ êµ¬ì„±ëœ ì²˜ë°©ì„ ë‚´ë¦¬ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ **ì¹˜ë£Œ íš¨ëŠ¥ì´ ê°•í•œ ì•½ì´ˆ(ì²œê¶, ë‹¹ê·€, ê°ˆê·¼ ë“±)**ë¥¼ ë©”ì¸ìœ¼ë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
     - ì£¼ì¦ìƒê³¼ ê´€ë ¨ ì—†ëŠ” ì•½ì´ˆ(ë‹¨ì§€ ì²´ì§ˆë§Œ ë§ëŠ” ê²½ìš°)ëŠ” ì œì™¸í•˜ì„¸ìš”.
   - **ì™¸ìš©ì•½**: ì£¼ì¦ìƒ ì™„í™”ì— ë„ì›€ì´ ë˜ëŠ” ì•½ì´ˆ ì„ íƒ. (ìê·¹ì„± ì•½ì¬ ì–¼êµ´ ë„í¬ ê¸ˆì§€)

2. ë‹µë³€ í¬ë§· (4ë‹¨ê³„):

   **1. ğŸ©º ì •ë°€ ì§„ë‹¨ê³¼ ë³‘ë¦¬ ë¶„ì„**
   - "í™˜ìë¶„ì€ [ì²´ì§ˆ]ì— ê°€ê¹Œìš°ë‚˜, í˜„ì¬ í˜¸ì†Œí•˜ì‹œëŠ” í†µì¦ì€ [êµ¬ì²´ì  ì–‘ìƒ]ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì´ëŠ” [í•œì˜í•™ì  ì›ì¸ ì¶”ë¡ ]ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
   
   **2. ğŸµ ë‚´ë³µìš”ë²• (ì¹˜ë£Œ ì¤‘ì‹¬)**
   - ì„ íƒëœ ì•½ì´ˆ 2~3ê°€ì§€ë¥¼ ì†Œê°œí•˜ì„¸ìš”.
   - **ì²˜ë°© ê·¼ê±°**: "ì´ ì•½ì´ˆë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” [ì¦ìƒ]ì„ [ì–´ë–»ê²Œ] ì¹˜ë£Œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤"ë¼ê³  ì „ë¬¸ê°€ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”. (ë‹¨ìˆœ ë‚˜ì—´ ê¸ˆì§€)
   - íƒ•ì•½/ì°¨ ë ˆì‹œí”¼ ì œì•ˆ.

   **3. ğŸ©¹ ì™¸ìš©ìš”ë²• (ì•ˆì „ ì œì¼)**
   - [External Context] í™œìš©. ì•ˆì „í•œ ì°œì§ˆ/ë„í¬ë²• ì œì•ˆ.

   **4. ğŸ§˜ ìƒí™œìš”ë²•**
   - ì¦ìƒ ì™„í™”ë¥¼ ìœ„í•œ êµ¬ì²´ì  í–‰ë™.

3. **ì•ˆì „ ê²½ê³ **: "ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì •í™•í•œ ì§„ë‹¨ì€ í•œì˜ì›ì—ì„œ ë°›ìœ¼ì„¸ìš”"ë¼ê³  ë§ë¶™ì´ì„¸ìš”.
4. **ì£¼ì˜**: **ì˜¤ì§ í•œêµ­ì–´(Korean)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.**
"""

# --- Main App ---

# Page Config
st.set_page_config(page_title="ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ", page_icon="ğŸŒ¿", layout="centered")

# Custom CSS
st.markdown("""
<style>
    .stApp { background-color: #f6f7f2; color: #2e3b28; }
    h1 { color: #4a5d23; font-family: 'Malgun Gothic', 'Apple SD Gothic Neo', sans-serif; text-align: center; margin-bottom: 2rem; }
    .stChatMessage { border-radius: 12px; padding: 1rem; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
    div[data-testid="stChatMessageContent"] { font-family: 'Malgun Gothic', 'Apple SD Gothic Neo', sans-serif; line-height: 1.6; }
    .stButton>button { background-color: #6b8c42 !important; color: white !important; border-radius: 20px; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸŒ¿ ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ")
st.warning("âš ï¸ ë³¸ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì˜í•™ì  ì§„ë‹¨ì„ ëŒ€ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Session State Initialization
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Initial Greeting
    greeting = "ë°˜ê°‘ìŠµë‹ˆë‹¤. AI í•œì˜ì‚¬ì…ë‹ˆë‹¤. \n\nì˜¤ëŠ˜ ì–´ë–¤ ë¶ˆí¸í•¨ ë•Œë¬¸ì— ì°¾ì•„ì˜¤ì…¨ëŠ”ì§€ìš”? ì¦ìƒì„ ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œë©´, ì œ ê¼¼ê¼¼í•˜ê²Œ ì‚´í´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    st.session_state.messages.append({"role": "assistant", "content": greeting})
    
if "turn_count" not in st.session_state:
    st.session_state.turn_count = 0

if "diagnosis_mode" not in st.session_state:
    st.session_state.diagnosis_mode = False

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User Input
if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. Add User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # 2. Logic Branching
    
    # Branch A: Interview Mode (Turns 0 ~ INTERVIEW_TURNS-1)
    if st.session_state.turn_count < INTERVIEW_TURNS:
        with st.chat_message("assistant"):
            with st.spinner("ì¦ìƒì„ ì‚´í”¼ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                response_text = generate_gemini_response(
                    st.session_state.messages, 
                    PROMPT_INTERVIEW
                )
                st.markdown(response_text)
        
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        st.session_state.turn_count += 1
        
        # Check if next turn should be diagnosis
        if st.session_state.turn_count >= INTERVIEW_TURNS:
             st.info("ğŸ’¡ ì¶©ë¶„í•œ ì •ë³´ê°€ ëª¨ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì •ë°€ ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # Branch B: Diagnosis Mode (Turn >= INTERVIEW_TURNS)
    else:
        with st.chat_message("assistant"):
            status_text = st.empty()
            status_text.text("ğŸ” ì •ë°€ ë¶„ì„ ì¤‘: ë‚´ë³µì•½ê³¼ ì™¸ìš©ì•½ì„ ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
            
            # 1. Refine Search Query (Dual)
            transcript = "\\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
            
            refine_prompt = PROMPT_QUERY_REFINEMENT_DUAL.format(history=transcript)
            
            raw_queries = generate_gemini_response([{"role": "user", "content": refine_prompt}], "")
            
            # Parse Queries
            try:
                # Naive splitting by newline
                lines = [line.strip() for line in raw_queries.strip().split('\n') if line.strip()]
                query_internal = lines[0] if len(lines) > 0 else "ê±´ê°• ìƒë‹´"
                query_external = lines[1] if len(lines) > 1 else query_internal
            except:
                query_internal = "ê±´ê°• ìƒë‹´"
                query_external = "ê±´ê°• ìƒë‹´"

            status_text.text(f"ğŸ” ê²€ìƒ‰ì–´ ìƒì„±:\n1. ë‚´ë³µ: {query_internal}\n2. ì™¸ìš©: {query_external}")
            
            # 2. Retrieve (Dual RAG)
            context_internal = retrieve_context(query_internal, top_k=3)
            context_external = retrieve_context(query_external, top_k=3)
            
            status_text.text("ğŸ’Š ì•ˆì „ì„± ê²€ì¦ ë° ì²˜ë°© ì‘ì„± ì¤‘...")
            
            # 3. Generate Prescription
            if len(st.session_state.messages) > 1:
                original_symptom = st.session_state.messages[1]['content']
            else:
                original_symptom = "ì•Œ ìˆ˜ ì—†ìŒ"

            final_system_prompt = PROMPT_PRESCRIPTION_EXPERT.format(
                context_internal=context_internal, 
                context_external=context_external,
                chief_complaint=original_symptom
            )
            
            response_text = generate_gemini_response(
                st.session_state.messages, 
                final_system_prompt
            )
            
            status_text.empty()
            st.markdown(response_text)
                
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        
        # Reset capability?
        # st.button("ìƒˆë¡œìš´ ìƒë‹´ ì‹œì‘", on_click=lambda: st.session_state.clear())


