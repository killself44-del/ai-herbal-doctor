import streamlit as st
import os
import requests
import json
from pinecone import Pinecone
from dotenv import load_dotenv
import db  # ğŸ“‚ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ëª¨ë“ˆ

# 1. ë¡œì»¬ í™˜ê²½(.env) ë¡œë“œ
load_dotenv()

# 2. ì•ˆì „í•œ í‚¤ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_secret(key_name):
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except Exception:
        pass
    return os.getenv(key_name)

GOOGLE_API_KEY = get_secret("GOOGLE_API_KEY")
PINECONE_API_KEY = get_secret("PINECONE_API_KEY")

if not GOOGLE_API_KEY or not PINECONE_API_KEY:
    st.error("ğŸš¨ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! Streamlit Cloudì˜ [Settings] -> [Secrets]ì— í‚¤ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# Configure Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "herb-knowledge"
index = pc.Index(index_name)

# Constants
GEMINI_EMBED_MODEL = "models/text-embedding-004"
GEMINI_GEN_MODEL = "gemini-2.0-flash-exp"
INTERVIEW_TURNS = 4 # ì‹¬ì¸µ ë¬¸ì§„ íšŸìˆ˜

# --- Helper Functions (ê¸°ì¡´ ê³ ê¸‰ ë¡œì§ ìœ ì§€) ---

def get_gemini_embedding(text):
    url = f"https://generativelanguage.googleapis.com/v1beta/{GEMINI_EMBED_MODEL}:embedContent?key={GOOGLE_API_KEY}"
    payload = {"model": GEMINI_EMBED_MODEL, "content": {"parts": [{"text": text}]}}
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']['values']
    except Exception as e:
        return None

def generate_gemini_response(messages, system_instruction):
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_GEN_MODEL}:generateContent?key={GOOGLE_API_KEY}"
    
    formatted_contents = []
    for msg in messages:
        role = "user" if msg["role"] == "user" else "model"
        formatted_contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })

    payload = {
        "system_instruction": {
            "parts": [{"text": system_instruction}]
        },
        "contents": formatted_contents
    }
    
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Error: {e}"

def retrieve_context(query, top_k=3):
    try:
        embedding = get_gemini_embedding(query)
        if not embedding: return ""
        
        search_results = index.query(
            vector=embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        contexts = []
        for match in search_results['matches']:
            meta = match['metadata']
            text = f"- ì´ë¦„: {meta.get('name', 'Unknown')}\n  íš¨ëŠ¥: {meta.get('efficacy', 'ì •ë³´ ì—†ìŒ')}\n  ì‚¬ìš©ë²•: {meta.get('usage', 'ì •ë³´ ì—†ìŒ')}\n  ì£¼ì˜ì‚¬í•­: {meta.get('caution', 'ì •ë³´ ì—†ìŒ')}"
            contexts.append(text)
            
        return "\n\n".join(contexts)
    except Exception as e:
        return ""

# --- System Prompts (ì—…ê·¸ë ˆì´ë“œ: êµí†µì •ë¦¬ & DB ë°˜ì˜) ---

PROMPT_QUERY_REFINEMENT_DUAL = """
ë‹¹ì‹ ì€ 'ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
Pinecone ê²€ìƒ‰ì„ ìœ„í•´ **ë‚´ë³µì•½ìš©**ê³¼ **ì™¸ìš©ì•½ìš©** ë‘ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ê°ê° ìƒì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. **Line 1 (ë‚´ë³µì•½)**: ì¦ìƒ ìƒì„¸(80%) + ì²´ì§ˆ ë³´ì™„(20%)
   - ì˜ˆ: "ë’·ëª© ë»£ë»£í•¨ ê¸´ì¥ì„± ë‘í†µ ê°ˆê·¼ (ì†ŒìŒì¸ ëª¸ì´ ì°¸)"
2. **Line 2 (ì™¸ìš©ì•½)**: ì¦ìƒ ì™„í™” + ì°œì§ˆ/ì•„ë¡œë§ˆ í‚¤ì›Œë“œ
   - ì˜ˆ: "ë‘í†µ ì¿¨ë§ êµ­í™” ë°•í•˜ ì°œì§ˆ (ì§„ì • íš¨ê³¼)"

[ëŒ€í™” ë‚´ìš©]
{history}

[ê²°ê³¼]
ì„¤ëª… ì—†ì´ ì˜¤ì§ **ë‘ ì¤„ì˜ ë¬¸ìì—´ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
"""

# â­ [í•µì‹¬ ìˆ˜ì •] ê³¼ê±° ê¸°ë¡ ë°˜ì˜ + êµí†µì •ë¦¬(Traffic Control) ê¸°ëŠ¥ ì¶”ê°€
PROMPT_INTERVIEW = """
ë‹¹ì‹ ì€ ë§¤ìš° ê¼¼ê¼¼í•˜ê³  ì‹ ì¤‘í•œ 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” **[ì‹¬ì¸µ ë¬¸ì§„(Deep Interview) ë‹¨ê³„]**ì…ë‹ˆë‹¤.

[ì°¸ê³ : í™˜ìì˜ ê³¼ê±° ì§„ë£Œ ê¸°ë¡]
{history_context}

[ì§€ì¹¨]
1. **ê¸°ì¡´ ì¦ìƒ í™•ì¸**: í™˜ìê°€ "ì•„ì§ ì•„íŒŒìš”"ë¼ê³  í•˜ë©´ ê³¼ê±° ì²˜ë°©ì´ íš¨ê³¼ê°€ ì—†ì—ˆìŒì„ ì¸ì§€í•˜ê³  ì›ì¸ì„ ì¬ë¶„ì„í•˜ì„¸ìš”.
2. **ìƒˆë¡œìš´ ì¦ìƒ í™•ì¸**: í™˜ìê°€ "ë‹¤ë¥¸ ê³³ì´ ì•„íŒŒìš”"ë¼ê³  í•˜ë©´ ìƒˆë¡œìš´ ë¬¸ì§„ì„ ì‹œì‘í•˜ì„¸ìš”.

â­ **[ì¤‘ìš”: ë³µí•© ì¦ìƒ ëŒ€ì²˜ (Traffic Control)]**
ë§Œì•½ í™˜ìê°€ **"ê¸°ì¡´ ë³‘ë„ ì•ˆ ë‚˜ì•˜ê³ , ìƒˆë¡œìš´ ë³‘ë„ ìƒê²¼ë‹¤"**ê³  ë™ì‹œì— í˜¸ì†Œí•˜ë©´:
- ë¬´ë¦¬í•˜ê²Œ ë‘ ê°€ì§€ë¥¼ í•œêº¼ë²ˆì— ì²˜ë°©í•˜ë ¤ í•˜ì§€ ë§ˆì„¸ìš”.
- **"ì €ëŸ°, ì—ì¹œ ë° ë®ì¹œ ê²©ì´êµ°ìš”. ë‘ ê°€ì§€ë¥¼ ë‹¤ ê³ ë ¤í•˜ê² ì§€ë§Œ, ì§€ê¸ˆ ë‹¹ì¥ ë” ê²¬ë””ê¸° í˜ë“¤ê±°ë‚˜ ì‹œê¸‰í•œ ì¦ìƒ í•˜ë‚˜ë¥¼ ë¨¼ì € ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ê·¸ë˜ì•¼ ë” ì •í™•í•˜ê³  ê°•ë ¥í•œ ì²˜ë°©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."**ë¼ê³  ë§í•˜ë©° **ìš°ì„ ìˆœìœ„**ë¥¼ ì •í•˜ê²Œ ìœ ë„í•˜ì„¸ìš”.

[ë¬¸ì§„ ìˆœì„œ]
1. **Turn 1**: ì£¼ì¦ìƒ(ê°€ì¥ ë¶ˆí¸í•œ ê²ƒ) í™•ì • ë° í†µì¦ ì–‘ìƒ íŒŒì•…
2. **Turn 2**: ì•…í™”/ì™„í™” ìš”ì¸ ë° ë°œë³‘ ì‹œê¸°
3. **Turn 3~4**: ì „ì‹  ìƒíƒœ(ì†Œí™”, ëŒ€ë³€, ì¶”ìœ„íƒ, ìˆ˜ë©´) íŒŒì•…í•˜ì—¬ ì²´ì§ˆ ì¶”ë¡ 
"""

PROMPT_PRESCRIPTION_EXPERT = """
ë‹¹ì‹ ì€ ëª…ì˜(åé†«) 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” ì§„ë‹¨ ë° ì²˜ë°© ë‹¨ê³„ì…ë‹ˆë‹¤. **ë‚´ë³µì•½**ê³¼ **ì™¸ìš©ì•½**ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë°©í•˜ì„¸ìš”.

[ì…ë ¥ ë°ì´í„°]
1. ë‚´ë³µì•½ í›„ë³´(Internal Context): {context_internal}
2. ì™¸ìš©ì•½ í›„ë³´(External Context): {context_external}
3. í™˜ì ì£¼ì¦ìƒ: {chief_complaint}

[ì§€ì¹¨]
1. **ê²€ì¦ ë‹¨ê³„(Self-Reflection)**:
   - **ë‚´ë³µì•½**: ì£¼ì¦ìƒì— íš¨ê³¼ê°€ ìˆê³  ì²´ì§ˆì— ë§ëŠ” ì•½ì´ˆ ì„ íƒ.
   - **ğŸ”´ ì ˆëŒ€ ê¸ˆê¸°**: 
     - ì‹í’ˆì„± ì•½ì¬(ìŒ€, ëŒ€ì¶” ë“±)ë§Œìœ¼ë¡œ ì²˜ë°© ê¸ˆì§€. ì¹˜ë£Œ íš¨ëŠ¥ì´ ê°•í•œ ì•½ì´ˆ í¬í•¨ í•„ìˆ˜.
   - **ì™¸ìš©ì•½**: ì£¼ì¦ìƒ ì™„í™”ì— ë„ì›€ì´ ë˜ëŠ” ì•½ì´ˆ ì„ íƒ.

2. ë‹µë³€ í¬ë§· (4ë‹¨ê³„):
   **1. ğŸ©º ì •ë°€ ì§„ë‹¨ê³¼ ë³‘ë¦¬ ë¶„ì„**
   **2. ğŸµ ë‚´ë³µìš”ë²• (ì¹˜ë£Œ ì¤‘ì‹¬)**
   **3. ğŸ©¹ ì™¸ìš©ìš”ë²• (ì•ˆì „ ì œì¼)**
   **4. ğŸ§˜ ìƒí™œìš”ë²•**

3. **ì•ˆì „ ê²½ê³ **: "ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤" ë¬¸êµ¬ í¬í•¨.
4. **ì£¼ì˜**: ì˜¤ì§ í•œêµ­ì–´(Korean)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. DB ì €ì¥ ê´€ë ¨ ë©˜íŠ¸ëŠ” í•˜ì§€ ë§ˆì„¸ìš”.
"""

# --- Main App ---

st.set_page_config(page_title="ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ", page_icon="ğŸŒ¿", layout="centered")

# Custom CSS
st.markdown("""
<style>
    .stApp { background-color: #f6f7f2; color: #2e3b28; }
    h1 { color: #4a5d23; font-family: 'Malgun Gothic', sans-serif; text-align: center; margin-bottom: 2rem; }
    .stChatMessage { border-radius: 12px; padding: 1rem; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
    .stButton>button { background-color: #6b8c42 !important; color: white !important; border-radius: 20px; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸŒ¿ ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ")

# --- [ì¶”ê°€ ê¸°ëŠ¥ 1] ë¡œê·¸ì¸ ì‹œìŠ¤í…œ ---
if "patient_id" not in st.session_state:
    st.session_state.patient_id = None

if not st.session_state.patient_id:
    with st.form("login_form"):
        st.subheader("ğŸ“‹ ì§„ë£Œ ì ‘ìˆ˜")
        p_id = st.text_input("ì„±í•¨ì´ë‚˜ ì „í™”ë²ˆí˜¸ ë’·ìë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: í™ê¸¸ë™1234")
        if st.form_submit_button("ìƒë‹´ ì‹œì‘"):
            if p_id:
                st.session_state.patient_id = p_id
                st.rerun()
    st.warning("âš ï¸ ë³¸ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì˜í•™ì  ì§„ë‹¨ì„ ëŒ€ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ë¡œê·¸ì¸ ì„±ê³µ í›„ ë¡œì§
p_id = st.session_state.patient_id
st.sidebar.success(f"í™˜ì: {p_id}ë‹˜ ì ‘ì† ì¤‘")

# ì´ˆê¸°í™” ë° DB ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
if "messages" not in st.session_state:
    st.session_state.messages = []
    
    # DB ì¡°íšŒ
    history = db.get_patient_history(p_id)
    
    if history:
        last = history[-1]
        st.session_state.history_context = f"- ìµœê·¼ë°©ë¬¸: {last['ë‚ ì§œ']}\n- ë‹¹ì‹œì¦ìƒ: {last['ì¦ìƒ']}\n- ë‹¹ì‹œì²˜ë°©: {last['ì²˜ë°©ì•½ì¬']}"
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜. ì§€ë‚œë²ˆ({last['ë‚ ì§œ']})ì—” **'{last['ì¦ìƒ']}'** ë¬¸ì œë¡œ ì²˜ë°©ì„ ë°›ìœ¼ì…¨ë„¤ìš”. ê·¸ê°„ ì°¨ë„ëŠ” ì¢€ ìˆìœ¼ì…¨ìŠµë‹ˆê¹Œ? ì˜¤ëŠ˜ ë¶ˆí¸í•˜ì‹  ê³³ì€ ì–´ë””ì¸ì§€ìš”?"
    else:
        st.session_state.history_context = "ê³¼ê±° ì§„ë£Œ ê¸°ë¡ ì—†ìŒ (ì‹ ê·œ í™˜ì)"
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜, AI í•œì˜ì‚¬ì…ë‹ˆë‹¤.\n\nì˜¤ëŠ˜ ì–´ë–¤ ë¶ˆí¸í•¨ ë•Œë¬¸ì— ì°¾ì•„ì˜¤ì…¨ëŠ”ì§€ìš”? ì¦ìƒì„ ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œë©´ ê¼¼ê¼¼í•˜ê²Œ ì‚´í´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    
    st.session_state.messages.append({"role": "assistant", "content": greeting})

if "turn_count" not in st.session_state:
    st.session_state.turn_count = 0
if "diagnosis_complete" not in st.session_state:
    st.session_state.diagnosis_complete = False

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # [Branch A] ì‹¬ì¸µ ë¬¸ì§„ ëª¨ë“œ (Turn 1~4)
    if st.session_state.turn_count < INTERVIEW_TURNS:
        with st.chat_message("assistant"):
            with st.spinner("ì¦ìƒì„ ì‚´í”¼ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                # í”„ë¡¬í”„íŠ¸ì— DB ê¸°ë¡(history_context) ì£¼ì…
                final_interview_prompt = PROMPT_INTERVIEW.format(history_context=st.session_state.history_context)
                
                response_text = generate_gemini_response(
                    st.session_state.messages, 
                    final_interview_prompt
                )
                st.markdown(response_text)
        
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        st.session_state.turn_count += 1
        
        if st.session_state.turn_count >= INTERVIEW_TURNS:
             st.info("ğŸ’¡ ì¶©ë¶„í•œ ì •ë³´ê°€ ëª¨ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì •ë°€ ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # [Branch B] ì •ë°€ ì§„ë‹¨ ëª¨ë“œ (Turn >= 4)
    else:
        if not st.session_state.diagnosis_complete:
            with st.chat_message("assistant"):
                status_text = st.status("ğŸ” ì •ë°€ ë¶„ì„ ì¤‘: ë‚´ë³µì•½ê³¼ ì™¸ìš©ì•½ì„ ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤...", expanded=True)
                
                # 1. ì¿¼ë¦¬ ìµœì í™” (Dual Query)
                transcript = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
                refine_prompt = PROMPT_QUERY_REFINEMENT_DUAL.format(history=transcript)
                raw_queries = generate_gemini_response([{"role": "user", "content": refine_prompt}], "")
                
                try:
                    lines = [line.strip() for line in raw_queries.strip().split('\n') if line.strip()]
                    query_internal = lines[0] if len(lines) > 0 else "ê±´ê°• ìƒë‹´"
                    query_external = lines[1] if len(lines) > 1 else query_internal
                except:
                    query_internal = "ê±´ê°• ìƒë‹´"; query_external = "ê±´ê°• ìƒë‹´"

                status_text.write(f"ğŸ” ê²€ìƒ‰ì–´ ìƒì„±:\n1. ë‚´ë³µ: {query_internal}\n2. ì™¸ìš©: {query_external}")
                
                # 2. Retrieve (Dual RAG)
                context_internal = retrieve_context(query_internal, top_k=3)
                context_external = retrieve_context(query_external, top_k=3)
                status_text.write("ğŸ’Š ì•ˆì „ì„± ê²€ì¦ ë° ì²˜ë°© ì‘ì„± ì¤‘...")
                
                # 3. Generate Prescription
                if len(st.session_state.messages) > 1:
                    # ë³´í†µ ë‘ ë²ˆì§¸ ë©”ì‹œì§€ê°€ ì£¼ì¦ìƒ
                    original_symptom = st.session_state.messages[1]['content']
                else:
                    original_symptom = "ì•Œ ìˆ˜ ì—†ìŒ"

                final_system_prompt = PROMPT_PRESCRIPTION_EXPERT.format(
                    context_internal=context_internal, 
                    context_external=context_external,
                    chief_complaint=original_symptom
                )
                
                response_text = generate_gemini_response(
                    st.session_state.messages, 
                    final_system_prompt
                )
                
                status_text.empty()
                st.markdown(response_text)
                
                # --- [ì¶”ê°€ ê¸°ëŠ¥ 2] ìë™ ì €ì¥ ---
                # ì§„ë‹¨ ê²°ê³¼ ì•ë¶€ë¶„ë§Œ ìš”ì•½í•´ì„œ ì €ì¥ (ë„ˆë¬´ ê¸¸ë©´ ì…€ì´ í„°ì§€ë‹ˆê¹Œ)
                if db.save_diagnosis(p_id, original_symptom, "AI ì •ë°€ ì§„ë‹¨", response_text[:300]+"..."):
                    st.success("ğŸ’¾ ì§„ë£Œ ê¸°ë¡ì´ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âš ï¸ ì €ì¥ ì‹¤íŒ¨ (ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”)")
                
            st.session_state.messages.append({"role": "assistant", "content": response_text})
            st.session_state.diagnosis_complete = True
