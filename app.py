import streamlit as st
import os
import requests
import json
from pinecone import Pinecone
from dotenv import load_dotenv
import db  # ğŸ“‚ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ëª¨ë“ˆ (ê¸°ì¡´ì— ì“°ì‹œë˜ ê²ƒ)

# 1. ë¡œì»¬ í™˜ê²½(.env) ë¡œë“œ
load_dotenv()

# 2. ì•ˆì „í•œ í‚¤ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_secret(key_name):
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except Exception:
        pass
    return os.getenv(key_name)

GOOGLE_API_KEY = get_secret("GOOGLE_API_KEY")
PINECONE_API_KEY = get_secret("PINECONE_API_KEY")

if not GOOGLE_API_KEY or not PINECONE_API_KEY:
    st.error("ğŸš¨ API í‚¤ ì˜¤ë¥˜: .env íŒŒì¼ì´ë‚˜ Secretsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# Pinecone ì„¤ì •
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "herb-knowledge"
index = pc.Index(index_name)

# ëª¨ë¸ ì„¤ì •
GEMINI_EMBED_MODEL = "models/text-embedding-004"
GEMINI_GEN_MODEL = "gemini-2.0-flash-exp"
INTERVIEW_TURNS = 3 # ë¬¸ì§„ íšŸìˆ˜ ì¡°ì ˆ

# --- Helper Functions ---

def get_gemini_embedding(text):
    # ğŸŒŸ í•µì‹¬: taskTypeì„ 'RETRIEVAL_QUERY'ë¡œ ì„¤ì • (ì´ê±´ ê²€ìƒ‰ ì§ˆë¬¸ì´ì•¼! ë¼ê³  ëª…ì‹œ)
    url = f"https://generativelanguage.googleapis.com/v1beta/{GEMINI_EMBED_MODEL}:embedContent?key={GOOGLE_API_KEY}"
    payload = {
        "model": GEMINI_EMBED_MODEL,
        "content": {"parts": [{"text": text}]},
        "taskType": "RETRIEVAL_QUERY" 
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']['values']
    except Exception as e:
        return None

def generate_gemini_response(messages, system_instruction):
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_GEN_MODEL}:generateContent?key={GOOGLE_API_KEY}"
    
    formatted_contents = []
    for msg in messages:
        role = "user" if msg["role"] == "user" else "model"
        formatted_contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })

    payload = {
        "system_instruction": {
            "parts": [{"text": system_instruction}]
        },
        "contents": formatted_contents
    }
    
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Error: {e}"

def retrieve_context(query, top_k=5):
    try:
        # ê²€ìƒ‰ì–´ ë³´ê°•: ë‹¨ìˆœíˆ 'ì„¤ì‚¬'ê°€ ì•„ë‹ˆë¼ 'ì„¤ì‚¬ ì¦ìƒì— ì¢‹ì€ ì•½ì´ˆ'ë¡œ ë³€í™˜
        enhanced_query = f"ì¦ìƒ '{query}'ì— íš¨ëŠ¥ì´ ìˆëŠ” ì•½ì´ˆ ì •ë³´"
        
        embedding = get_gemini_embedding(enhanced_query)
        if not embedding: return ""
        
        search_results = index.query(
            vector=embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        contexts = []
        for match in search_results['matches']:
            meta = match['metadata']
            # ì´ë¦„ê³¼ íš¨ëŠ¥ ìœ„ì£¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            text = f"- ì•½ì´ˆëª…: {meta.get('name', 'Unknown')}\n  íš¨ëŠ¥: {meta.get('efficacy', 'ì •ë³´ ì—†ìŒ')}\n  ì£¼ì˜ì‚¬í•­: {meta.get('caution', 'ì •ë³´ ì—†ìŒ')}"
            contexts.append(text)
            
        return "\n\n".join(contexts)
    except Exception as e:
        return ""

# --- System Prompts (.replace ë°©ì‹ ì‚¬ìš©ìœ¼ë¡œ ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨) ---

PROMPT_QUERY_REFINEMENT_DUAL = """
ë‹¹ì‹ ì€ 'ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
Pinecone ê²€ìƒ‰ì„ ìœ„í•´ **ë‚´ë³µì•½ìš©**ê³¼ **ì™¸ìš©ì•½ìš©** ë‘ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ê°ê° ìƒì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. **Line 1 (ë‚´ë³µì•½)**: ì¦ìƒ ìƒì„¸ + ì²´ì§ˆ ê³ ë ¤ í‚¤ì›Œë“œ
2. **Line 2 (ì™¸ìš©ì•½)**: ì¦ìƒ ì™„í™” + ì°œì§ˆ/ì•„ë¡œë§ˆ í‚¤ì›Œë“œ

[ëŒ€í™” ë‚´ìš©]
__HISTORY__

[ê²°ê³¼]
ì„¤ëª… ì—†ì´ ì˜¤ì§ **ë‘ ì¤„ì˜ ë¬¸ìì—´ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
"""

PROMPT_INTERVIEW = """
ë‹¹ì‹ ì€ 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤. 
í™˜ìì˜ ê³¼ê±° ê¸°ë¡(__HISTORY_CONTEXT__)ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ì§„ì„ ì§„í–‰í•˜ì„¸ìš”.

[ì§€ì¹¨]
1. ê³¼ê±°ì— ë°©ë¬¸í•œ ì ì´ ìˆë‹¤ë©´, "ì§€ë‚œë²ˆ __OLD_SYMPTOM__ ì¦ìƒì€ ì–´ë– ì‹ ê°€ìš”?"ë¼ê³  ì•ˆë¶€ë¥¼ ë¨¼ì € ë¬¼ìœ¼ì„¸ìš”.
2. í™˜ìì˜ í˜„ì¬ ë¶ˆí¸í•œ ì¦ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ì§ˆë¬¸í•˜ì„¸ìš”. (3íšŒ ì´ë‚´)
3. ë¶ˆí•„ìš”í•œ ì¸ì‚¬ëŠ” ìƒëµí•˜ê³  í•µì‹¬ë§Œ ì§ˆë¬¸í•˜ì„¸ìš”.
"""

PROMPT_PRESCRIPTION_EXPERT = """
ë‹¹ì‹ ì€ ëª…ì˜(åé†«) 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê²€ìƒ‰ëœ ì•½ì´ˆ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ í™˜ìì—ê²Œ ì²˜ë°©ì„ ë‚´ë¦¬ì„¸ìš”.

[ê²€ìƒ‰ëœ ì•½ì´ˆ ì •ë³´]
__CONTEXT_INTERNAL__
__CONTEXT_EXTERNAL__

[í™˜ì ì¦ìƒ]
__CHIEF_COMPLAINT__

[ì§€ì¹¨]
1. ê²€ìƒ‰ëœ ì•½ì´ˆ ì¤‘ì—ì„œ í™˜ìì˜ ì¦ìƒì— ê°€ì¥ ì í•©í•œ ê²ƒì„ ê³¨ë¼ **ë‚´ë³µì•½**ê³¼ **ì™¸ìš©ë²•**ì„ ì¶”ì²œí•˜ì„¸ìš”.
2. ì•½ì´ˆì˜ ì´ë¦„ê³¼ íš¨ëŠ¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë©° ì„¤ëª…í•˜ì„¸ìš”.
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ë”°ëœ»í•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

# --- Main App ---

st.set_page_config(page_title="ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ", page_icon="ğŸŒ¿", layout="centered")
st.markdown("<style>.stApp { background-color: #f6f7f2; color: #2e3b28; }</style>", unsafe_allow_html=True)
st.title("ğŸŒ¿ ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ")

# ë¡œê·¸ì¸
if "patient_id" not in st.session_state:
    st.session_state.patient_id = None

if not st.session_state.patient_id:
    with st.form("login_form"):
        p_id = st.text_input("ì„±í•¨/ì „í™”ë²ˆí˜¸ ì…ë ¥", placeholder="ì˜ˆ: í™ê¸¸ë™1234")
        if st.form_submit_button("ìƒë‹´ ì‹œì‘"):
            if p_id:
                st.session_state.patient_id = p_id
                st.rerun()
    st.stop()

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
p_id = st.session_state.patient_id
if "messages" not in st.session_state:
    st.session_state.messages = []
    
    history = db.get_patient_history(p_id)
    if history:
        last = history[-1]
        st.session_state.history_context = f"ìµœê·¼ë°©ë¬¸: {last['ë‚ ì§œ']}, ì¦ìƒ: {last['ì¦ìƒ']}"
        st.session_state.old_symptom = last['ì¦ìƒ']
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜. ì§€ë‚œë²ˆ {last['ì¦ìƒ']} ì¦ìƒì€ ì¢€ ì–´ë– ì‹ ê°€ìš”? ì˜¤ëŠ˜ì€ ì–´ë””ê°€ ë¶ˆí¸í•˜ì‹ ê°€ìš”?"
    else:
        st.session_state.history_context = "ì‹ ê·œ í™˜ì"
        st.session_state.old_symptom = "ì—†ìŒ"
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜. ì˜¤ëŠ˜ ì–´ë””ê°€ ë¶ˆí¸í•´ì„œ ì˜¤ì…¨ë‚˜ìš”?"
    
    st.session_state.messages.append({"role": "assistant", "content": greeting})

if "turn_count" not in st.session_state:
    st.session_state.turn_count = 0
if "diagnosis_complete" not in st.session_state:
    st.session_state.diagnosis_complete = False

# ì±„íŒ… í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ë¬¸ì§„ ë‹¨ê³„
    if st.session_state.turn_count < INTERVIEW_TURNS:
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                final_prompt = PROMPT_INTERVIEW.replace("__HISTORY_CONTEXT__", st.session_state.history_context)
                final_prompt = final_prompt.replace("__OLD_SYMPTOM__", st.session_state.old_symptom)
                
                response = generate_gemini_response(st.session_state.messages, final_prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.turn_count += 1
        
        if st.session_state.turn_count >= INTERVIEW_TURNS:
             st.info("ğŸ’¡ ì§„ë£Œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ì •ë³´ê°€ ëª¨ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

    # ì§„ë‹¨ ë‹¨ê³„
    else:
        if not st.session_state.diagnosis_complete:
            with st.chat_message("assistant"):
                status = st.status("ğŸ” ì•½ì´ˆ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...", expanded=True)
                
                # ê²€ìƒ‰ì–´ ìƒì„±
                transcript = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
                refine_prompt = PROMPT_QUERY_REFINEMENT_DUAL.replace("__HISTORY__", transcript)
                queries = generate_gemini_response([{"role": "user", "content": refine_prompt}], "")
                
                try:
                    q_lines = queries.strip().split('\n')
                    q_int = q_lines[0]
                    q_ext = q_lines[1] if len(q_lines) > 1 else q_int
                except:
                    q_int = "ì¦ìƒ ì™„í™” ì•½ì´ˆ"
                    q_ext = "ì¦ìƒ ì™„í™” ì•½ì´ˆ"
                
                status.write(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {q_int} / {q_ext}")

                # ê²€ìƒ‰ ì‹¤í–‰ (ì—¬ê¸°ê°€ ì¤‘ìš”!)
                ctx_int = retrieve_context(q_int)
                ctx_ext = retrieve_context(q_ext)
                
                status.write("ì²˜ë°©ì „ ì‘ì„± ì¤‘...")
                
                # ì²˜ë°© ìƒì„±
                symptom = st.session_state.messages[1]['content'] if len(st.session_state.messages) > 1 else "ì•Œ ìˆ˜ ì—†ìŒ"
                final_prompt = PROMPT_PRESCRIPTION_EXPERT.replace("__CONTEXT_INTERNAL__", ctx_int)
                final_prompt = final_prompt.replace("__CONTEXT_EXTERNAL__", ctx_ext)
                final_prompt = final_prompt.replace("__CHIEF_COMPLAINT__", symptom)
                
                diagnosis = generate_gemini_response(st.session_state.messages, final_prompt)
                
                status.empty()
                st.markdown(diagnosis)
                
                # ì €ì¥
                db.save_diagnosis(p_id, symptom, "AI ì§„ë‹¨", diagnosis[:200])
                
            st.session_state.messages.append({"role": "assistant", "content": diagnosis})
            st.session_state.diagnosis_complete = True
            
            if st.button("ìƒˆë¡œìš´ ìƒë‹´"):
                st.session_state.clear()
                st.rerun()
