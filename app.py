import streamlit as st
import os
import requests
import json
from pinecone import Pinecone
from dotenv import load_dotenv
import db  # ğŸ“‚ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ëª¨ë“ˆ

# 1. ë¡œì»¬ í™˜ê²½(.env) ë¡œë“œ
load_dotenv()

# 2. ì•ˆì „í•œ í‚¤ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_secret(key_name):
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except Exception:
        pass
    return os.getenv(key_name)

GOOGLE_API_KEY = get_secret("GOOGLE_API_KEY")
PINECONE_API_KEY = get_secret("PINECONE_API_KEY")

if not GOOGLE_API_KEY or not PINECONE_API_KEY:
    st.error("ğŸš¨ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! Streamlit Cloudì˜ [Settings] -> [Secrets]ì— í‚¤ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# Configure Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "herb-knowledge"
index = pc.Index(index_name)

# Constants
GEMINI_EMBED_MODEL = "models/text-embedding-004"
GEMINI_GEN_MODEL = "gemini-2.0-flash-exp"
INTERVIEW_TURNS = 4 # ì‹¬ì¸µ ë¬¸ì§„ íšŸìˆ˜

# --- Helper Functions (ê¸°ì¡´ ê³ ê¸‰ ë¡œì§ ìœ ì§€) ---

def get_gemini_embedding(text):
    url = f"https://generativelanguage.googleapis.com/v1beta/{GEMINI_EMBED_MODEL}:embedContent?key={GOOGLE_API_KEY}"
    payload = {"model": GEMINI_EMBED_MODEL, "content": {"parts": [{"text": text}]}}
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['embedding']['values']
    except Exception as e:
        return None

def generate_gemini_response(messages, system_instruction):
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_GEN_MODEL}:generateContent?key={GOOGLE_API_KEY}"
    
    formatted_contents = []
    for msg in messages:
        role = "user" if msg["role"] == "user" else "model"
        formatted_contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })

    payload = {
        "system_instruction": {
            "parts": [{"text": system_instruction}]
        },
        "contents": formatted_contents
    }
    
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Error: {e}"

def retrieve_context(query, top_k=3):
    try:
        embedding = get_gemini_embedding(query)
        if not embedding: return ""
        
        search_results = index.query(
            vector=embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        contexts = []
        for match in search_results['matches']:
            meta = match['metadata']
            text = f"- ì´ë¦„: {meta.get('name', 'Unknown')}\n  íš¨ëŠ¥: {meta.get('efficacy', 'ì •ë³´ ì—†ìŒ')}\n  ì‚¬ìš©ë²•: {meta.get('usage', 'ì •ë³´ ì—†ìŒ')}\n  ì£¼ì˜ì‚¬í•­: {meta.get('caution', 'ì •ë³´ ì—†ìŒ')}"
            contexts.append(text)
            
        return "\n\n".join(contexts)
    except Exception as e:
        return ""

# --- System Prompts (ì—…ê·¸ë ˆì´ë“œ: êµí†µì •ë¦¬ & DB ë°˜ì˜) ---

PROMPT_QUERY_REFINEMENT_DUAL = """
ë‹¹ì‹ ì€ 'ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
Pinecone ê²€ìƒ‰ì„ ìœ„í•´ **ë‚´ë³µì•½ìš©**ê³¼ **ì™¸ìš©ì•½ìš©** ë‘ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ê°ê° ìƒì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. **Line 1 (ë‚´ë³µì•½)**: ì¦ìƒ ìƒì„¸(80%) + ì²´ì§ˆ ë³´ì™„(20%)
   - ì˜ˆ: "ë’·ëª© ë»£ë»£í•¨ ê¸´ì¥ì„± ë‘í†µ ê°ˆê·¼ (ì†ŒìŒì¸ ëª¸ì´ ì°¸)"
2. **Line 2 (ì™¸ìš©ì•½)**: ì¦ìƒ ì™„í™” + ì°œì§ˆ/ì•„ë¡œë§ˆ í‚¤ì›Œë“œ
   - ì˜ˆ: "ë‘í†µ ì¿¨ë§ êµ­í™” ë°•í•˜ ì°œì§ˆ (ì§„ì • íš¨ê³¼)"

[ëŒ€í™” ë‚´ìš©]
{history}

[ê²°ê³¼]
ì„¤ëª… ì—†ì´ ì˜¤ì§ **ë‘ ì¤„ì˜ ë¬¸ìì—´ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.
"""

# â­ [í•µì‹¬ ìˆ˜ì •] ê³¼ê±° ê¸°ë¡ ë°˜ì˜ + êµí†µì •ë¦¬(Traffic Control) ê¸°ëŠ¥ ì¶”ê°€
# (ìˆ˜ì •ë¨) ë¬¸ì§„ í”„ë¡¬í”„íŠ¸: ì²´ì§ˆ ê¸°ì–µ & ì™„ì¹˜ ì—¬ë¶€ í™•ì¸ ê¸°ëŠ¥ ê°•í™”
# (ìˆ˜ì •ë¨) ë¬¸ì§„ í”„ë¡¬í”„íŠ¸: ì—ëŸ¬ ìˆ˜ì •ë³¸ (old_symptom ë³€ìˆ˜ ì œê±°)
PROMPT_INTERVIEW = """
ë‹¹ì‹ ì€ ê¸°ì–µë ¥ì´ ì¢‹ê³  ìœµí†µì„± ìˆëŠ” 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” **[ì‹¬ì¸µ ë¬¸ì§„(Deep Interview) ë‹¨ê³„]**ì…ë‹ˆë‹¤.

[í™˜ìì˜ ê³¼ê±° ì§„ë£Œ ê¸°ë¡]
{history_context}

[ì§€ì¹¨ 1: ì²´ì§ˆ ì •ë³´ ì¬ì‚¬ìš© (ì¤‘ë³µ ì§ˆë¬¸ ê¸ˆì§€)]
- ê³¼ê±° ê¸°ë¡ì— í™˜ìì˜ **ì²´ì§ˆ(ì˜ˆ: ì†ŒìŒì¸, ëª¸ì´ ì°¸, ì—´ì´ ë§ìŒ ë“±)**ì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‹¤ë©´, ì´ë²ˆ ë¬¸ì§„ì—ì„œëŠ” ì†Œí™”/ëŒ€ë³€/í•œì—´(ì¶”ìœ„íƒ) ë“±ì˜ **ì²´ì§ˆ í™•ì¸ ì§ˆë¬¸ì„ ìƒëµ**í•˜ì„¸ìš”.
- ëŒ€ì‹  "í™˜ìë¶„ì€ í‰ì†Œ ëª¸ì´ ì°¨ì‹  í¸ì´ë‹ˆ..."ì™€ ê°™ì´ **ì•Œê³  ìˆë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰**í•˜ë©° ê³µê°ëŒ€ë¥¼ í˜•ì„±í•˜ì„¸ìš”.

[ì§€ì¹¨ 2: ì‹ ê·œ/êµ¬ ì¦ìƒ êµí†µì •ë¦¬ (Traffic Control)]
í™˜ìê°€ ì˜¤ëŠ˜ **ìƒˆë¡œìš´ ì¦ìƒ(New)**ì„ ì´ì•¼ê¸°í–ˆì„ ë•Œ:
1. **ì™„ì¹˜ ì—¬ë¶€ í™•ì¸ (ìµœìš°ì„ )**: AIê°€ ì„ì˜ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ë°˜ë“œì‹œ **"ê·¸ëŸ°ë° ì§€ë‚œë²ˆ ë¶ˆí¸í•˜ì…¨ë˜ ì¦ìƒì€ ì´ì œ ê¹¨ë—ì´ ë‚˜ìœ¼ì…¨ìŠµë‹ˆê¹Œ?"**ë¼ê³  ë¨¼ì € ë¬¼ì–´ë³´ì„¸ìš”. (ê¸°ë¡ëœ ê³¼ê±° ì¦ìƒëª…ì„ ì–¸ê¸‰í•˜ë©° ë¬¼ì–´ë³´ì„¸ìš”)
2. **Case A (ì™„ì¹˜ë¨)**: í™˜ìê°€ "ë‚˜ì•˜ë‹¤"ê³  í•˜ë©´, ê³¼ê±° ì¦ìƒì€ ìŠê³  **ì˜¤ì§ ìƒˆë¡œìš´ ì¦ìƒ**ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì§„ë‹¨í•˜ì„¸ìš”. (ê³¼ê±° ì²˜ë°©ì— êµ¬ì• ë°›ì§€ ë§ˆì„¸ìš”)
3. **Case B (ì•ˆ ë‚˜ìŒ)**: í™˜ìê°€ "ì•„ì§ ì•ˆ ë‚˜ì•˜ë‹¤"ê³  í•˜ë©´, ê·¸ë•Œ ë¹„ë¡œì†Œ **"ë‘ ê°€ì§€ ë‹¤ ì¹˜ë£Œí•´ì•¼ê² êµ°ìš”. í•˜ì§€ë§Œ ì•½íš¨ ì§‘ì¤‘ì„ ìœ„í•´ ë‹¹ì¥ ë” ê´´ë¡œìš´ ê²ƒ í•˜ë‚˜ë§Œ ë¨¼ì € ê¼½ì•„ì£¼ì‹œê² ì–´ìš”?"**ë¼ê³  ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ì„¸ìš”.

[ë¬¸ì§„ ì§„í–‰ ìˆœì„œ]
1. **Turn 1**: ì£¼ì¦ìƒ íŒŒì•… ë° **ê³¼ê±° ì¦ìƒ ì™„ì¹˜ ì—¬ë¶€ í™•ì¸** (í•„ìˆ˜)
2. **Turn 2**: í†µì¦ì˜ ìƒì„¸ ì–‘ìƒ ë° ì•…í™” ìš”ì¸
3. **Turn 3**: (ê³¼ê±° ê¸°ë¡ì— ì²´ì§ˆ ì •ë³´ê°€ **ì—†ì„ ë•Œë§Œ**) ì „ì‹  ìƒíƒœ/ì²´ì§ˆ íŒŒì•…
   - *ì£¼ì˜*: ì²´ì§ˆ ì •ë³´ê°€ ì´ë¯¸ ìˆë‹¤ë©´ Turn 3ëŠ” ìƒëµí•˜ê³  ë°”ë¡œ ì§„ë‹¨ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê² ë‹¤ê³  ë§í•˜ì„¸ìš”.
"""

# (ìˆ˜ì •ë¨) ì²˜ë°© í”„ë¡¬í”„íŠ¸: ì²´ì§ˆ ê¸°ë¡ ì €ì¥ ê°•í™”
PROMPT_PRESCRIPTION_EXPERT = """
ë‹¹ì‹ ì€ ëª…ì˜(åé†«) 'AI í•œì˜ì‚¬'ì…ë‹ˆë‹¤.
í˜„ì¬ ë‹¨ê³„ëŠ” ì§„ë‹¨ ë° ì²˜ë°© ë‹¨ê³„ì…ë‹ˆë‹¤.

[ì…ë ¥ ë°ì´í„°]
1. ë‚´ë³µì•½ í›„ë³´: {context_internal}
2. ì™¸ìš©ì•½ í›„ë³´: {context_external}
3. í™˜ì ì£¼ì¦ìƒ: {chief_complaint}

[ì§€ì¹¨]
1. **ì²´ì§ˆ ëª…ì‹œ**: ì§„ë‹¨ ë‚´ìš©ì— ë°˜ë“œì‹œ í™˜ìì˜ **ì¶”ì • ì²´ì§ˆ(ì˜ˆ: ì†ŒìŒì¸ ê²½í–¥, ëª¸ì´ ì°¬ ì²´ì§ˆ ë“±)**ì„ í…ìŠ¤íŠ¸ë¡œ ë‚¨ê¸°ì„¸ìš”. (ë‹¤ìŒ ì§„ë£Œ ë•Œ ê¸°ì–µí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤)
2. **ì¦ìƒ ë¶„ë¦¬**: ë§Œì•½ í™˜ìê°€ "ê³¼ê±° ì¦ìƒì€ ë‹¤ ë‚˜ì•˜ë‹¤"ê³  í–ˆë‹¤ë©´, ê³¼ê±° ì¦ìƒìš© ì•½ì¬ëŠ” ë¹¼ê³  **ì˜¤ì§ ì˜¤ëŠ˜ ì¦ìƒ**ì— ë§ëŠ” ì•½ì¬ë§Œ ì²˜ë°©í•˜ì„¸ìš”.
3. ë‹µë³€ í¬ë§· (4ë‹¨ê³„):
   **1. ğŸ©º ì •ë°€ ì§„ë‹¨ (ì²´ì§ˆ ë¶„ì„ í¬í•¨)**
      - "í™˜ìë¶„ì˜ ê¸°ë¡ê³¼ ì¦ìƒì„ ì¢…í•©í•  ë•Œ [ì²´ì§ˆ]ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤..."
   **2. ğŸµ ë‚´ë³µìš”ë²• (ì¹˜ë£Œ ì¤‘ì‹¬)**
   **3. ğŸ©¹ ì™¸ìš©ìš”ë²• (ì•ˆì „ ì œì¼)**
   **4. ğŸ§˜ ìƒí™œìš”ë²•**

4. **ì£¼ì˜**: ì˜¤ì§ í•œêµ­ì–´(Korean)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

# --- Main App ---

st.set_page_config(page_title="ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ", page_icon="ğŸŒ¿", layout="centered")

# Custom CSS
st.markdown("""
<style>
    .stApp { background-color: #f6f7f2; color: #2e3b28; }
    h1 { color: #4a5d23; font-family: 'Malgun Gothic', sans-serif; text-align: center; margin-bottom: 2rem; }
    .stChatMessage { border-radius: 12px; padding: 1rem; box-shadow: 0 2px 5px rgba(0,0,0,0.05); }
    .stButton>button { background-color: #6b8c42 !important; color: white !important; border-radius: 20px; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸŒ¿ ì‹¬ì¸µ ì•½ì´ˆ ìƒë‹´ì†Œ")

# --- [ì¶”ê°€ ê¸°ëŠ¥ 1] ë¡œê·¸ì¸ ì‹œìŠ¤í…œ ---
if "patient_id" not in st.session_state:
    st.session_state.patient_id = None

if not st.session_state.patient_id:
    with st.form("login_form"):
        st.subheader("ğŸ“‹ ì§„ë£Œ ì ‘ìˆ˜")
        p_id = st.text_input("ì„±í•¨ì´ë‚˜ ì „í™”ë²ˆí˜¸ ë’·ìë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: í™ê¸¸ë™1234")
        if st.form_submit_button("ìƒë‹´ ì‹œì‘"):
            if p_id:
                st.session_state.patient_id = p_id
                st.rerun()
    st.warning("âš ï¸ ë³¸ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì˜í•™ì  ì§„ë‹¨ì„ ëŒ€ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ë¡œê·¸ì¸ ì„±ê³µ í›„ ë¡œì§
p_id = st.session_state.patient_id
st.sidebar.success(f"í™˜ì: {p_id}ë‹˜ ì ‘ì† ì¤‘")

# ì´ˆê¸°í™” ë° DB ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
if "messages" not in st.session_state:
    st.session_state.messages = []
    
    # DB ì¡°íšŒ
    history = db.get_patient_history(p_id)
    
    if history:
        last = history[-1]
        st.session_state.history_context = f"- ìµœê·¼ë°©ë¬¸: {last['ë‚ ì§œ']}\n- ë‹¹ì‹œì¦ìƒ: {last['ì¦ìƒ']}\n- ë‹¹ì‹œì²˜ë°©: {last['ì²˜ë°©ì•½ì¬']}"
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜. ì§€ë‚œë²ˆ({last['ë‚ ì§œ']})ì—” **'{last['ì¦ìƒ']}'** ë¬¸ì œë¡œ ì²˜ë°©ì„ ë°›ìœ¼ì…¨ë„¤ìš”. ê·¸ê°„ ì°¨ë„ëŠ” ì¢€ ìˆìœ¼ì…¨ìŠµë‹ˆê¹Œ? ì˜¤ëŠ˜ ë¶ˆí¸í•˜ì‹  ê³³ì€ ì–´ë””ì¸ì§€ìš”?"
    else:
        st.session_state.history_context = "ê³¼ê±° ì§„ë£Œ ê¸°ë¡ ì—†ìŒ (ì‹ ê·œ í™˜ì)"
        greeting = f"ë°˜ê°‘ìŠµë‹ˆë‹¤ {p_id}ë‹˜, AI í•œì˜ì‚¬ì…ë‹ˆë‹¤.\n\nì˜¤ëŠ˜ ì–´ë–¤ ë¶ˆí¸í•¨ ë•Œë¬¸ì— ì°¾ì•„ì˜¤ì…¨ëŠ”ì§€ìš”? ì¦ìƒì„ ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œë©´ ê¼¼ê¼¼í•˜ê²Œ ì‚´í´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    
    st.session_state.messages.append({"role": "assistant", "content": greeting})

if "turn_count" not in st.session_state:
    st.session_state.turn_count = 0
if "diagnosis_complete" not in st.session_state:
    st.session_state.diagnosis_complete = False

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # [Branch A] ì‹¬ì¸µ ë¬¸ì§„ ëª¨ë“œ (Turn 1~4)
    if st.session_state.turn_count < INTERVIEW_TURNS:
        with st.chat_message("assistant"):
            with st.spinner("ì¦ìƒì„ ì‚´í”¼ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                # í”„ë¡¬í”„íŠ¸ì— DB ê¸°ë¡(history_context) ì£¼ì…
                final_interview_prompt = PROMPT_INTERVIEW.format(history_context=st.session_state.history_context)
                
                response_text = generate_gemini_response(
                    st.session_state.messages, 
                    final_interview_prompt
                )
                st.markdown(response_text)
        
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        st.session_state.turn_count += 1
        
        if st.session_state.turn_count >= INTERVIEW_TURNS:
             st.info("ğŸ’¡ ì¶©ë¶„í•œ ì •ë³´ê°€ ëª¨ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì •ë°€ ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # [Branch B] ì •ë°€ ì§„ë‹¨ ëª¨ë“œ (Turn >= 4)
    else:
        if not st.session_state.diagnosis_complete:
            with st.chat_message("assistant"):
                status_text = st.status("ğŸ” ì •ë°€ ë¶„ì„ ì¤‘: ë‚´ë³µì•½ê³¼ ì™¸ìš©ì•½ì„ ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤...", expanded=True)
                
                # 1. ì¿¼ë¦¬ ìµœì í™” (Dual Query)
                transcript = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
                refine_prompt = PROMPT_QUERY_REFINEMENT_DUAL.format(history=transcript)
                raw_queries = generate_gemini_response([{"role": "user", "content": refine_prompt}], "")
                
                try:
                    lines = [line.strip() for line in raw_queries.strip().split('\n') if line.strip()]
                    query_internal = lines[0] if len(lines) > 0 else "ê±´ê°• ìƒë‹´"
                    query_external = lines[1] if len(lines) > 1 else query_internal
                except:
                    query_internal = "ê±´ê°• ìƒë‹´"; query_external = "ê±´ê°• ìƒë‹´"

                status_text.write(f"ğŸ” ê²€ìƒ‰ì–´ ìƒì„±:\n1. ë‚´ë³µ: {query_internal}\n2. ì™¸ìš©: {query_external}")
                
                # 2. Retrieve (Dual RAG)
                context_internal = retrieve_context(query_internal, top_k=3)
                context_external = retrieve_context(query_external, top_k=3)
                status_text.write("ğŸ’Š ì•ˆì „ì„± ê²€ì¦ ë° ì²˜ë°© ì‘ì„± ì¤‘...")
                
                # 3. Generate Prescription
                if len(st.session_state.messages) > 1:
                    # ë³´í†µ ë‘ ë²ˆì§¸ ë©”ì‹œì§€ê°€ ì£¼ì¦ìƒ
                    original_symptom = st.session_state.messages[1]['content']
                else:
                    original_symptom = "ì•Œ ìˆ˜ ì—†ìŒ"

                final_system_prompt = PROMPT_PRESCRIPTION_EXPERT.format(
                    context_internal=context_internal, 
                    context_external=context_external,
                    chief_complaint=original_symptom
                )
                
                response_text = generate_gemini_response(
                    st.session_state.messages, 
                    final_system_prompt
                )
                
                status_text.empty()
                st.markdown(response_text)
                
                # --- [ì¶”ê°€ ê¸°ëŠ¥ 2] ìë™ ì €ì¥ ---
                # ì§„ë‹¨ ê²°ê³¼ ì•ë¶€ë¶„ë§Œ ìš”ì•½í•´ì„œ ì €ì¥ (ë„ˆë¬´ ê¸¸ë©´ ì…€ì´ í„°ì§€ë‹ˆê¹Œ)
                if db.save_diagnosis(p_id, original_symptom, "AI ì •ë°€ ì§„ë‹¨", response_text[:300]+"..."):
                    st.success("ğŸ’¾ ì§„ë£Œ ê¸°ë¡ì´ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âš ï¸ ì €ì¥ ì‹¤íŒ¨ (ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”)")
                
            st.session_state.messages.append({"role": "assistant", "content": response_text})
            st.session_state.diagnosis_complete = True


